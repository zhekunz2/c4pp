import pyro, numpy as np, torch, pyro.distributions   as dist, torch.nn as nn
from pyro.optim import Adam
import torch.distributions.constraints as constraints
from pyro.infer import SVI
if pyro.__version__ > '0.1.2': from pyro.infer import Trace_ELBO
from pyro.contrib.autoguide import *
import math
def amb(x):
    return x.data.numpy().tolist() if isinstance(x, torch.Tensor) else x
height= np.array([74.0, 66.0, 64.0, 63.0, 63.0, 64.0, 62.0, 73.0, 72.0, 72.0, 70.0, 63.0, 68.0, 68.0, 65.0, 60.0, 66.0, 68.0, 68.0, 70.0, 67.0, 64.0, 73.0, 62.0, 63.0, 67.0, 66.0, 72.0, 63.0, 68.0, 64.0, 60.0, 72.0, 68.0, 77.0, 64.0, 64.0, 64.0, 72.0, 68.0, 64.0, 61.0, 67.0, 65.0, 64.0, 67.0, 65.0, 65.0, 67.0, 68.0, 70.0, 60.0, 65.0, 62.0, 72.0, 67.0, 70.0, 70.0, 68.0, 60.0, 71.0, 71.0, 73.0, 68.0, 66.0, 69.0, 63.0, 63.0, 66.0, 64.0, 62.0, 64.0, 64.0, 67.0, 71.0, 60.0, 66.0, 66.0, 65.0, 70.0, 65.0, 66.0, 62.0, 66.0, 64.0, 72.0, 67.0, 68.0, 70.0, 62.0, 63.0, 66.0, 70.0, 68.0, 70.0, 66.0, 66.0, 68.0, 70.0, 61.0, 63.0, 64.0, 67.0, 61.0, 69.0, 64.0, 68.0, 64.0, 67.0, 69.0, 67.0, 68.0, 63.0, 71.0, 64.0, 68.0, 68.0, 66.0, 66.0, 66.0, 77.0, 67.0, 62.0, 64.0, 72.0, 67.0, 61.0, 64.0, 62.0, 63.0, 68.0, 64.0, 72.0, 64.0, 61.0, 74.0, 70.0, 71.0, 63.0, 64.0, 62.0, 62.0, 70.0, 62.0, 71.0, 71.0, 70.0, 67.0, 64.0, 67.0, 62.0, 62.0, 68.0, 62.0, 66.0, 68.0, 74.0, 68.0, 62.0, 66.0, 68.0, 60.0, 68.0, 72.0, 66.0, 68.0, 68.0, 69.0, 64.0, 65.0, 61.0, 72.0, 66.0, 62.0, 60.0, 68.0, 70.0, 71.0, 71.0, 68.0, 66.0, 63.0, 60.0, 63.0, 68.0, 64.0, 70.0, 68.0, 61.0, 68.0, 66.0, 70.0, 71.0, 65.0, 66.0, 70.0, 64.0, 63.0, 62.0, 66.0, 61.0, 72.0, 74.0, 67.0, 66.0, 62.0, 62.0, 72.0, 65.0, 60.0, 62.0, 66.0, 64.0, 71.0, 65.0, 73.0, 69.0, 64.0, 64.0, 61.0, 67.0, 62.0, 63.0, 67.0, 68.0, 72.0, 70.0, 70.0, 67.0, 63.0, 66.0, 66.0, 71.0, 66.0, 67.0, 76.0, 69.0, 65.0, 62.0, 62.0, 62.0, 62.0, 61.0, 65.0, 72.0, 66.0, 72.0, 64.0, 71.0, 72.0, 66.0, 65.0, 65.0, 62.0, 66.0, 67.0, 64.0, 63.0, 67.0, 68.0, 64.0, 64.0, 66.0, 65.0, 65.0, 72.0, 68.0, 63.0, 68.0, 74.0, 63.0, 64.0, 68.0, 67.0, 67.0, 73.0, 62.0, 70.0, 65.0, 69.0, 77.0, 68.0, 62.0, 64.0, 67.0, 70.0, 62.0, 63.0, 62.0, 63.0, 69.0, 64.0, 64.0, 69.0, 59.0, 66.0, 65.0, 62.0, 75.0, 63.0, 65.0, 70.0, 68.0, 65.0, 69.0, 72.0, 64.0, 63.0, 64.0, 66.0, 63.0, 73.0, 75.0, 65.0, 66.0, 66.0, 64.0, 66.0, 64.0, 63.0, 63.0, 73.0, 72.0, 66.0, 72.0, 64.0, 64.0, 68.0, 62.0, 69.0, 60.0, 65.0, 76.0, 72.0, 62.0, 62.0, 71.0, 74.0, 65.0, 63.0, 67.0, 65.0, 63.0, 65.0, 64.0, 65.0, 68.0, 63.0, 70.0, 68.0, 67.0, 65.0, 69.0, 68.0, 66.0, 71.0, 64.0, 74.0, 62.0, 65.0, 69.0, 66.0, 60.0, 67.0, 73.0, 65.0, 76.0, 70.0, 64.0, 63.0, 63.0, 70.0, 71.0, 65.0, 70.0, 63.0, 64.0, 64.0, 67.0, 66.0, 68.0, 65.0, 67.0, 73.0, 64.0, 71.0, 63.0, 71.0, 63.0, 73.0, 65.0, 66.0, 66.0, 59.0, 70.0, 72.0, 64.0, 66.0, 70.0, 69.0, 66.0, 68.0, 64.0, 63.0, 74.0, 73.0, 67.0, 64.0, 67.0, 70.0, 64.0, 62.0, 66.0, 69.0, 68.0, 72.0, 63.0, 63.0, 69.0, 64.0, 71.0, 64.0, 72.0, 62.0, 66.0, 66.0, 61.0, 69.0, 68.0, 66.0, 68.0, 64.0, 61.0, 66.0, 62.0, 65.0, 66.0, 72.0, 63.0, 63.0, 70.0, 62.0, 71.0, 70.0, 64.0, 63.0, 65.0, 71.0, 62.0, 73.0, 68.0, 76.0, 66.0, 64.0, 60.0, 68.0, 61.0, 66.0, 63.0, 63.0, 74.0, 70.0, 68.0, 70.0, 69.0, 70.0, 63.0, 65.0, 69.0, 68.0, 62.0, 66.0, 65.0, 70.0, 66.0, 70.0, 66.0, 68.0, 68.0, 70.0, 65.0, 64.0, 65.0, 77.0, 70.0, 64.0, 64.0, 62.0, 64.0, 64.0, 63.0, 58.0, 66.0, 62.0, 61.0, 64.0, 64.0, 62.0, 64.0, 66.0, 66.0, 69.0, 64.0, 62.0, 71.0, 62.0, 68.0, 68.0, 62.0, 63.0, 69.0, 67.0, 64.0, 70.0, 69.0, 69.0, 69.0, 60.0, 66.0, 60.0, 67.0, 66.0, 70.0, 64.0, 60.0, 68.0, 73.0, 72.0, 64.0, 67.0, 67.0, 71.0, 63.0, 73.0, 70.0, 70.0, 71.0, 66.0, 75.0, 60.0, 72.0, 63.0, 75.0, 60.0, 73.0, 65.0, 67.0, 61.0, 69.0, 64.0, 70.0, 68.0, 62.0, 64.0, 68.0, 68.0, 66.0, 64.0, 73.0, 73.0, 69.0, 68.0, 71.0, 63.0, 66.0, 66.0, 66.0, 68.0, 70.0, 62.0, 66.0, 63.0, 75.0, 67.0, 62.0, 63.0, 65.0, 69.0, 71.0, 62.0, 65.0, 72.0, 65.0, 63.0, 64.0, 72.0, 64.0, 70.0, 65.0, 64.0, 64.0, 67.0, 73.0, 73.0, 65.0, 64.0, 67.0, 67.0, 74.0, 63.0, 74.0, 74.0, 63.0, 62.0, 62.0, 67.0, 69.0, 73.0, 63.0, 60.0, 67.0, 69.0, 71.0, 72.0, 63.0, 70.0, 64.0, 64.0, 68.0, 74.0, 62.0, 65.0, 66.0, 60.0, 69.0, 61.0, 65.0, 66.0, 70.0, 72.0, 64.0, 71.0, 63.0, 73.0, 64.0, 68.0, 64.0, 66.0, 67.0, 64.0, 64.0, 62.0, 69.0, 63.0, 72.0, 61.0, 75.0, 70.0, 69.0, 64.0, 72.0, 69.0, 63.0, 71.0, 59.0, 68.0, 77.0, 68.0, 70.0, 69.0, 63.0, 73.0, 66.0, 72.0, 66.0, 72.0, 69.0, 67.0, 66.0, 67.0, 68.0, 65.0, 62.0, 69.0, 62.0, 70.0, 66.0, 66.0, 66.0, 72.0, 69.0, 66.0, 69.0, 60.0, 65.0, 62.0, 67.0, 67.0, 68.0, 65.0, 66.0, 74.0, 65.0, 63.0, 62.0, 68.0, 66.0, 65.0, 62.0, 73.0, 62.0, 70.0, 58.0, 68.0, 67.0, 64.0, 70.0, 72.0, 63.0, 63.0, 66.0, 65.0, 61.0, 61.0, 67.0, 73.0, 71.0, 64.0, 67.0, 62.0, 67.0, 69.0, 66.0, 66.0, 66.0, 66.0, 64.0, 72.0, 64.0, 64.0, 65.0, 72.0, 63.0, 71.0, 64.0, 64.0, 67.0, 75.0, 66.0, 67.0, 68.0, 71.0, 62.0, 68.0, 70.0, 67.0, 64.0, 71.0, 63.0, 64.0, 61.0, 65.0, 67.0, 65.0, 66.0, 67.0, 74.0, 74.0, 66.0, 69.0, 66.0, 74.0, 72.0, 66.0, 67.0, 65.0, 66.0, 65.0, 62.0, 68.0, 64.0, 64.0, 63.0, 69.0, 67.0, 63.0, 67.0, 60.0, 62.0, 72.0, 67.0, 67.0, 66.0, 66.0, 60.0, 67.0, 65.0, 69.0, 72.0, 64.0, 64.0, 62.0, 65.0, 64.0, 67.0, 66.0, 73.0, 74.0, 66.0, 65.0, 72.0, 64.0, 63.0, 63.0, 66.0, 72.0, 70.0, 66.0, 63.0, 73.0, 63.0, 69.0, 66.0, 73.0, 68.0, 64.0, 63.0, 70.0, 64.0, 66.0, 61.0, 72.0, 65.0, 64.0, 68.0, 74.0, 67.0, 70.0, 66.0, 71.0, 71.0, 71.0, 74.0, 75.0, 62.0, 72.0, 66.0, 65.0, 66.0, 72.0, 60.0, 64.0, 65.0, 65.0, 62.0, 68.0, 73.0, 63.0, 64.0, 66.0, 68.0, 63.0, 66.0, 71.0, 73.0, 64.0, 61.0, 63.0, 63.0, 62.0, 64.0, 67.0, 67.0, 61.0, 63.0, 73.0, 66.0, 70.0, 68.0, 67.0, 74.0, 64.0, 61.0, 66.0, 60.0, 59.0, 62.0, 62.0, 62.0, 75.0, 66.0, 64.0, 71.0, 61.0, 63.0, 67.0, 69.0, 70.0, 66.0, 74.0, 65.0, 64.0, 68.0, 66.0, 69.0, 60.0, 61.0, 75.0, 63.0, 69.0, 66.0, 64.0, 63.0, 67.0, 64.0, 72.0, 63.0, 67.0, 66.0, 65.0, 63.0, 66.0, 64.0, 64.0, 71.0, 71.0, 72.0, 60.0, 70.0, 74.0, 68.0, 74.0, 75.0, 68.0, 63.0, 72.0, 64.0, 63.0, 66.0, 61.0, 62.0, 73.0, 66.0, 68.0, 62.0, 65.0, 63.0, 62.0, 68.0, 59.0, 62.0, 64.0, 68.0, 63.0, 63.0, 68.0, 64.0, 70.0, 70.0, 72.0, 69.0, 63.0, 58.0, 63.0, 63.0, 73.0, 69.0, 70.0, 66.0, 66.0, 65.0, 72.0, 62.0, 69.0, 72.0, 73.0, 68.0, 64.0, 66.0, 60.0, 60.0, 65.0, 63.0, 64.0, 71.0, 67.0, 67.0, 63.0, 65.0, 66.0, 63.0, 65.0, 74.0, 67.0, 68.0, 65.0, 64.0, 68.0, 65.0, 72.0, 66.0, 64.0, 64.0, 60.0, 72.0, 64.0, 67.0, 68.0, 64.0, 66.0, 65.0, 70.0, 70.0, 66.0, 64.0, 66.0, 66.0, 74.0, 62.0, 66.0, 62.0, 65.0, 63.0, 71.0, 66.0, 62.0, 62.0, 63.0, 65.0, 67.0, 61.0, 66.0, 61.0, 72.0, 69.0, 64.0, 63.0, 60.0, 70.0, 73.0, 65.0, 64.0, 63.0, 63.0, 72.0, 67.0, 69.0, 71.0, 68.0, 73.0, 66.0, 63.0, 65.0, 71.0, 72.0, 68.0, 64.0, 72.0, 66.0, 66.0, 65.0, 65.0, 66.0, 65.0, 71.0, 74.0, 70.0, 64.0, 69.0, 66.0, 63.0, 68.0, 71.0, 73.0, 63.0, 65.0, 71.0, 65.0, 64.0, 65.0, 71.0, 70.0, 73.0, 68.0, 66.0, 64.0, 66.0, 63.0, 70.0, 64.0, 66.0, 63.0, 71.0, 64.0, 63.0, 66.0, 63.0, 68.0, 65.0, 61.0, 62.0, 69.0, 64.0, 70.0, 65.0, 61.0, 64.0, 65.0, 64.0, 69.0, 74.0, 68.0, 68.0, 65.0, 67.0, 65.0, 64.0, 62.0, 72.0, 73.0, 70.0, 64.0, 68.0, 63.0, 71.0, 64.0, 67.0, 60.0, 66.0, 59.0, 63.0, 66.0, 66.0, 65.0, 64.0, 66.0, 66.0, 73.0, 72.0, 74.0, 65.0, 64.0, 71.0, 68.0, 61.0, 70.0, 64.0, 59.0, 68.0, 68.0, 69.0, 69.0, 75.0, 69.0, 72.0, 68.0, 72.0, 72.0, 67.0, 63.0, 69.0, 63.0, 72.0, 65.0, 73.0, 69.0, 69.0, 61.0, 73.0, 67.0, 63.0, 66.0, 62.0, 65.0, 72.0, 67.0, 65.0, 72.0, 65.0, 64.0, 68.0, 68.0, 76.0, 63.0, 74.0, 68.0, 72.0, 68.0, 65.0, 60.0, 72.0, 64.0, 68.0, 61.0, 67.0, 67.0, 63.0, 63.0, 60.0, 74.0, 69.0, 67.0, 65.0, 62.0, 65.0, 72.0, 66.0, 65.0, 62.0, 71.0, 62.0, 71.0, 70.0, 67.0, 62.0, 63.0, 61.0, 72.0, 65.0, 63.0, 66.0, 64.0, 66.0, 68.0, 60.0, 65.0, 65.0, 61.0, 64.0, 66.0, 69.0, 66.0, 67.0, 68.0, 66.0, 69.0, 69.0, 71.0, 74.0, 69.0, 65.0, 65.0, 75.0, 72.0, 64.0, 59.0, 71.0, 72.0, 63.0, 68.0, 64.0, 70.0, 66.0, 62.0, 70.0, 64.0, 69.0, 64.0, 67.0, 72.0, 65.0, 63.0, 70.0, 64.0, 68.0, 66.0, 66.0, 70.0, 62.0, 66.0, 66.0, 64.0, 70.0, 74.0, 66.0, 62.0, 66.0, 62.0, 69.0, 68.0, 72.0, 64.0, 64.0, 60.0, 72.0, 62.0, 74.0, 66.0, 68.0, 62.0, 71.0, 63.0, 69.0, 71.0, 64.0, 66.0, 72.0, 62.0, 72.0, 68.0, 64.0, 64.0, 70.0, 66.0, 67.0, 68.0, 64.0, 65.0, 70.0, 71.0, 64.0, 62.0, 71.0, 74.0, 64.0, 70.0, 64.0, 70.0, 74.0, 63.0, 59.0, 74.0, 66.0, 65.0, 71.0, 69.0, 60.0, 64.0, 64.0, 72.0, 61.0, 62.0, 69.0, 67.0, 65.0, 68.0, 74.0, 63.0, 65.0, 71.0, 64.0, 65.0, 69.0, 64.0, 72.0, 63.0, 65.0, 72.0, 65.0, 64.0, 71.0, 63.0, 62.0, 66.0, 64.0, 67.0, 68.0, 66.0, 66.0, 66.0, 67.0, 66.0, 63.0, 68.0, 70.0, 63.0, 70.0, 72.0, 75.0, 62.0, 68.0, 68.0, 70.0, 73.0, 69.0, 62.0, 60.0, 64.0, 60.0, 73.0, 66.0, 69.0, 70.0, 70.0, 72.0, 61.0, 64.0, 72.0, 68.0], dtype=np.float32).reshape(1379,1)
height=torch.tensor(height)
male= np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0], dtype=np.float32).reshape(1379,1)
male=torch.tensor(male)
earn_pos= np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32).reshape(1379,1)
earn_pos=torch.tensor(earn_pos)
N=1379
N=torch.tensor(N)
def model(height,male,earn_pos,N):
    with pyro.iarange('beta_range_'.format('')):
        beta = pyro.sample('beta'.format(''), dist.Normal(torch.tensor(1234.0)*torch.ones([amb(3)]),torch.tensor(1234.0)*torch.ones([amb(3)])))
    sigma = pyro.sample('sigma'.format(''), dist.Normal(torch.tensor(1234.0)*torch.ones([amb(1)]),torch.tensor(1234.0)*torch.ones([amb(1)])))
    pyro.sample('obs__100'.format(), dist.Bernoulli(logits=beta[0-1]+beta[1-1]*height+beta[3-1]*male), obs=earn_pos)
    
def guide(height,male,earn_pos,N):
    arg_1 = pyro.param('arg_1', torch.ones((amb(3))), constraint=constraints.positive)
    arg_2 = pyro.param('arg_2', torch.ones((amb(3))), constraint=constraints.positive)
    with pyro.iarange('beta_prange'):
        beta = pyro.sample('beta'.format(''), dist.Beta(arg_1,arg_2))
    arg_3 = pyro.param('arg_3', torch.ones((amb(1))), constraint=constraints.positive)
    arg_4 = pyro.param('arg_4', torch.ones((amb(1))), constraint=constraints.positive)
    sigma = pyro.sample('sigma'.format(''), dist.Beta(arg_3,arg_4))
    
    pass
    return { "beta": beta,"sigma": sigma, }
optim = Adam({'lr': 0.05})
svi = SVI(model, guide, optim, loss=Trace_ELBO() if pyro.__version__ > '0.1.2' else 'ELBO')
for i in range(4000):
    loss = svi.step(height,male,earn_pos,N)
    if ((i % 1000) == 0):
        print(loss)
for name in pyro.get_param_store().get_all_param_names():
    print(('{0} : {1}'.format(name, pyro.param(name).data.numpy())))
print('beta_mean', np.array2string(dist.Beta(pyro.param('arg_1'), pyro.param('arg_2')).mean.detach().numpy(), separator=','))
print('sigma_mean', np.array2string(dist.Beta(pyro.param('arg_3'), pyro.param('arg_4')).mean.detach().numpy(), separator=','))
np.set_printoptions(threshold=np.inf)
with open('samples','w') as samplefile:
    samplefile.write('beta:')
    samplefile.write(np.array2string(np.array([guide(height,male,earn_pos,N)['beta'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('sigma:')
    samplefile.write(np.array2string(np.array([guide(height,male,earn_pos,N)['sigma'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
