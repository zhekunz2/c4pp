import pyro, numpy as np, torch, pyro.distributions   as dist, torch.nn as nn
from pyro.optim import Adam
import torch.distributions.constraints as constraints
from pyro.infer import SVI
if pyro.__version__ > '0.1.2': from pyro.infer import Trace_ELBO
from pyro.contrib.autoguide import *
import math
def amb(x):
    return x.data.numpy().tolist() if isinstance(x, torch.Tensor) else x
earn= np.array([50000.0, 60000.0, 30000.0, 51000.0, 9000.0, 29000.0, 32000.0, 2000.0, 27000.0, 30000.0, 12000.0, 12000.0, 22000.0, 17000.0, 40000.0, 44000.0, 7000.0, 53000.0, 5000.0, 14000.0, 5500.0, 40000.0, 34000.0, 10000.0, 27000.0, 50000.0, 41000.0, 15000.0, 25000.0, 75000.0, 27000.0, 12000.0, 30000.0, 21000.0, 27000.0, 25000.0, 24000.0, 32000.0, 10000.0, 11000.0, 18700.0, 20000.0, 3500.0, 13000.0, 21000.0, 34000.0, 17000.0, 35000.0, 4000.0, 14000.0, 10000.0, 25000.0, 16000.0, 16000.0, 16500.0, 3840.0, 22000.0, 200.0, 26000.0, 2500.0, 17000.0, 8000.0, 10000.0, 15000.0, 2400.0, 30000.0, 30000.0, 10000.0, 5000.0, 12000.0, 20000.0, 20000.0, 20000.0, 1200.0, 700.0, 20000.0, 10000.0, 40000.0, 60000.0, 16040.0, 15000.0, 10000.0, 33000.0, 18000.0, 15000.0, 21000.0, 21000.0, 37000.0, 38000.0, 17000.0, 32000.0, 27500.0, 16500.0, 25000.0, 27000.0, 5000.0, 70000.0, 5000.0, 5000.0, 20000.0, 4000.0, 60000.0, 5000.0, 30000.0, 70000.0, 50000.0, 44000.0, 30000.0, 10000.0, 23000.0, 45000.0, 15000.0, 17000.0, 30000.0, 27500.0, 18000.0, 43000.0, 32000.0, 10000.0, 60000.0, 21000.0, 2400.0, 1000.0, 27000.0, 6600.0, 16000.0, 90000.0, 8000.0, 20000.0, 15000.0, 12000.0, 24000.0, 20000.0, 19000.0, 10000.0, 40000.0, 25000.0, 25000.0, 25000.0, 19000.0, 44000.0, 15000.0, 17000.0, 24000.0, 23000.0, 65000.0, 7000.0, 40000.0, 15000.0, 20000.0, 20000.0, 20000.0, 25000.0, 49000.0, 11000.0, 16000.0, 35000.0, 125000.0, 23000.0, 17000.0, 27000.0, 70000.0, 35000.0, 10000.0, 35000.0, 15000.0, 12000.0, 8000.0, 35000.0, 45000.0, 15000.0, 24000.0, 25000.0, 25000.0, 20000.0, 24000.0, 44000.0, 69000.0, 62000.0, 32000.0, 20000.0, 32000.0, 25000.0, 170000.0, 35000.0, 40000.0, 33000.0, 18000.0, 30000.0, 26000.0, 5000.0, 17000.0, 32000.0, 15000.0, 50000.0, 8000.0, 40000.0, 40000.0, 32750.0, 36000.0, 6000.0, 12000.0, 60000.0, 40000.0, 43000.0, 45000.0, 6000.0, 20000.0, 17000.0, 2000.0, 65000.0, 50000.0, 11000.0, 35000.0, 27000.0, 3500.0, 42000.0, 20000.0, 15000.0, 10000.0, 17000.0, 28000.0, 15000.0, 20000.0, 20000.0, 10500.0, 13000.0, 10000.0, 3000.0, 24000.0, 17000.0, 11000.0, 32000.0, 17000.0, 3000.0, 2100.0, 3192.0, 17000.0, 30000.0, 15000.0, 24000.0, 50000.0, 50000.0, 20000.0, 30000.0, 22000.0, 27000.0, 4000.0, 17500.0, 16500.0, 28000.0, 52000.0, 15000.0, 19000.0, 27000.0, 15000.0, 15000.0, 14500.0, 24000.0, 18000.0, 4000.0, 700.0, 27000.0, 12000.0, 22000.0, 30000.0, 35000.0, 20000.0, 32000.0, 6000.0, 12000.0, 10000.0, 1000.0, 16000.0, 25000.0, 25000.0, 32000.0, 35000.0, 6000.0, 80000.0, 20000.0, 1000.0, 28000.0, 26000.0, 27000.0, 18000.0, 2000.0, 6000.0, 25000.0, 12000.0, 12000.0, 4000.0, 26000.0, 23000.0, 21000.0, 25000.0, 4000.0, 23900.0, 35000.0, 26000.0, 25000.0, 23500.0, 12000.0, 15000.0, 3000.0, 15000.0, 175000.0, 1000.0, 10000.0, 20000.0, 35000.0, 40000.0, 50000.0, 100000.0, 35000.0, 24000.0, 35000.0, 30000.0, 5000.0, 148000.0, 30000.0, 6500.0, 3000.0, 40000.0, 30000.0, 14000.0, 15000.0, 26000.0, 8000.0, 24000.0, 10000.0, 12000.0, 50000.0, 23000.0, 40000.0, 6000.0, 5000.0, 110000.0, 41000.0, 21000.0, 4000.0, 25000.0, 30000.0, 14000.0, 6000.0, 14000.0, 43000.0, 25000.0, 65000.0, 16000.0, 20000.0, 12000.0, 8000.0, 16000.0, 19000.0, 21000.0, 43000.0, 35000.0, 21000.0, 5800.0, 17000.0, 24000.0, 11000.0, 10000.0, 40000.0, 40000.0, 24000.0, 24000.0, 20000.0, 60000.0, 11000.0, 5000.0, 27000.0, 89000.0, 40000.0, 15000.0, 1000.0, 22000.0, 12000.0, 22000.0, 40000.0, 40000.0, 35000.0, 7000.0, 14000.0, 15000.0, 6000.0, 11000.0, 10000.0, 20000.0, 38000.0, 29000.0, 20000.0, 8000.0, 36000.0, 1500.0, 42000.0, 25000.0, 13000.0, 30000.0, 7000.0, 31000.0, 15000.0, 50000.0, 5000.0, 600.0, 14000.0, 40000.0, 24000.0, 22000.0, 20000.0, 20112.0, 41000.0, 13000.0, 60000.0, 24000.0, 38000.0, 3000.0, 13000.0, 5000.0, 20000.0, 22000.0, 28000.0, 22000.0, 30000.0, 30000.0, 30000.0, 26000.0, 22000.0, 25000.0, 11000.0, 13000.0, 10000.0, 12000.0, 7000.0, 57000.0, 36000.0, 29000.0, 8000.0, 3000.0, 20000.0, 33000.0, 16000.0, 8000.0, 32000.0, 14500.0, 13000.0, 7000.0, 12000.0, 19000.0, 9000.0, 20000.0, 55000.0, 10000.0, 13000.0, 20000.0, 30000.0, 12000.0, 26000.0, 12500.0, 8000.0, 15000.0, 65000.0, 25000.0, 8000.0, 15000.0, 12000.0, 35000.0, 40000.0, 29000.0, 15000.0, 20000.0, 3000.0, 1000.0, 8000.0, 5000.0, 25000.0, 20000.0, 30000.0, 12000.0, 10000.0, 16000.0, 45000.0, 40000.0, 19000.0, 17600.0, 30000.0, 18000.0, 25000.0, 6000.0, 70000.0, 15000.0, 25000.0, 35000.0, 18000.0, 28000.0, 15000.0, 22000.0, 35000.0, 18000.0, 35000.0, 16400.0, 22000.0, 30000.0, 17000.0, 25000.0, 10000.0, 40000.0, 12500.0, 35000.0, 105000.0, 100000.0, 21000.0, 10000.0, 33000.0, 26000.0, 18000.0, 15000.0, 5000.0, 60000.0, 28000.0, 42000.0, 2000.0, 20000.0, 28000.0, 10000.0, 31000.0, 17000.0, 16000.0, 35000.0, 3000.0, 16000.0, 35000.0, 27000.0, 25000.0, 40000.0, 45000.0, 14000.0, 25000.0, 40000.0, 34000.0, 40000.0, 35000.0, 12000.0, 30000.0, 23000.0, 2000.0, 20000.0, 10000.0, 60000.0, 12000.0, 25000.0, 8000.0, 2000.0, 18600.0, 20000.0, 100000.0, 50000.0, 27000.0, 1000.0, 18000.0, 17000.0, 14000.0, 33000.0, 23000.0, 10000.0, 22000.0, 13000.0, 33000.0, 36000.0, 10000.0, 6000.0, 12000.0, 10000.0, 21000.0, 26000.0, 22000.0, 23000.0, 20000.0, 10000.0, 7000.0, 15000.0, 26000.0, 10000.0, 3000.0, 27000.0, 28000.0, 15000.0, 55000.0, 20000.0, 8500.0, 30000.0, 20000.0, 25000.0, 3000.0, 16000.0, 10000.0, 15000.0, 28000.0, 31500.0, 95000.0, 38000.0, 30000.0, 35900.0, 12000.0, 10000.0, 45000.0, 40000.0, 21500.0, 14000.0, 38000.0, 1000.0, 15000.0, 24000.0, 4500.0, 18000.0, 14000.0, 10000.0, 9500.0, 6000.0, 2500.0, 40000.0, 25000.0, 28000.0, 33000.0, 4000.0, 4100.0, 32000.0, 13000.0, 8000.0, 17000.0, 18000.0, 12000.0, 7000.0, 1200.0, 14000.0, 17000.0, 30000.0, 25000.0, 22000.0, 15000.0, 20000.0, 9000.0, 17000.0, 1800.0, 18000.0, 25000.0, 20000.0, 75000.0, 3500.0, 30000.0, 1000.0, 9000.0, 11000.0, 12000.0, 30000.0, 19000.0, 9000.0, 30000.0, 9000.0, 15000.0, 8000.0, 33000.0, 12000.0, 18000.0, 30000.0, 12500.0, 10000.0, 21000.0, 21000.0, 5000.0, 22000.0, 30000.0, 123000.0, 8000.0, 2000.0, 7600.0, 10000.0, 35000.0, 15000.0, 4000.0, 16000.0, 200.0, 23000.0, 16000.0, 6000.0, 600.0, 28000.0, 15000.0, 15000.0, 25000.0, 15000.0, 40000.0, 30000.0, 5000.0, 35000.0, 16000.0, 28000.0, 20000.0, 21000.0, 17000.0, 24000.0, 40000.0, 8000.0, 19000.0, 5000.0, 24000.0, 5000.0, 33000.0, 24000.0, 6000.0, 90000.0, 57000.0, 13800.0, 30000.0, 15000.0, 31000.0, 33000.0, 30000.0, 12000.0, 5000.0, 21000.0, 28000.0, 12000.0, 19000.0, 6000.0, 5300.0, 15000.0, 17000.0, 1700.0, 5400.0, 10000.0, 35000.0, 28000.0, 42000.0, 45000.0, 55000.0, 36000.0, 25000.0, 19000.0, 38000.0, 15000.0, 96000.0, 11000.0, 9000.0, 5000.0, 50000.0, 6000.0, 100000.0, 16500.0, 32000.0, 30000.0, 10000.0, 15000.0, 26000.0, 17000.0, 22000.0, 10000.0, 50000.0, 20000.0, 400.0, 16000.0, 14000.0, 5000.0, 40000.0, 5000.0, 25000.0, 12000.0, 58000.0, 18000.0, 1000.0, 21000.0, 11000.0, 13000.0, 25000.0, 600.0, 25000.0, 36000.0, 15000.0, 3500.0, 19005.0, 62000.0, 25000.0, 50000.0, 40000.0, 26000.0, 22000.0, 21000.0, 30000.0, 27000.0, 4000.0, 45000.0, 18000.0, 6000.0, 14000.0, 35000.0, 15000.0, 12000.0, 1000.0, 32000.0, 1500.0, 3000.0, 35000.0, 70000.0, 21000.0, 50000.0, 15000.0, 9000.0, 10000.0, 30000.0, 16000.0, 20000.0, 100000.0, 10000.0, 35000.0, 7000.0, 8000.0, 11000.0, 25000.0, 25000.0, 30000.0, 48000.0, 17000.0, 26000.0, 10000.0, 60000.0, 22000.0, 24000.0, 20000.0, 7000.0, 30000.0, 23000.0, 20000.0, 3000.0, 30000.0, 12000.0, 40000.0, 15000.0, 36000.0, 10000.0, 13000.0, 10000.0, 30000.0, 23000.0, 28000.0, 20000.0, 25000.0, 25000.0, 18000.0, 16000.0, 45000.0, 12000.0, 2000.0, 10000.0, 14000.0, 15000.0, 10000.0, 1200.0, 15000.0, 10000.0, 2000.0, 4000.0, 10000.0, 1500.0, 18000.0, 4000.0, 28000.0, 10000.0, 3000.0, 12000.0, 19000.0, 36000.0, 7000.0, 35000.0, 40000.0, 15000.0, 8000.0, 25000.0, 28000.0, 22000.0, 21500.0, 3000.0, 23000.0, 16000.0, 15000.0, 53000.0, 26000.0, 4000.0, 10000.0, 43000.0, 17000.0, 4000.0, 50000.0, 55000.0, 28000.0, 5000.0, 18000.0, 21000.0, 12000.0, 33000.0, 50000.0, 16000.0, 35000.0, 25000.0, 20000.0, 18000.0, 25000.0, 40000.0, 14000.0, 12000.0, 25000.0, 38000.0, 14000.0, 15000.0, 17000.0, 18000.0, 70000.0, 30000.0, 8000.0, 30000.0, 27000.0, 47000.0, 12000.0, 24000.0, 13000.0, 80000.0, 5000.0, 25000.0, 28000.0, 8000.0, 10000.0, 9000.0, 20000.0, 20000.0, 28000.0, 265.0, 26000.0, 27000.0, 14000.0, 32000.0, 10000.0, 7000.0, 28000.0, 60000.0, 12000.0, 15000.0, 26000.0, 26000.0, 18720.0, 80000.0, 48000.0, 18000.0, 35000.0, 5000.0, 9000.0, 30000.0, 80000.0, 35000.0, 12000.0, 200000.0, 4000.0, 2000.0, 2000.0, 15000.0, 1500.0, 28000.0, 9000.0, 60000.0, 1200.0, 1400.0, 7000.0, 21000.0, 44000.0, 29000.0, 3000.0, 12000.0, 20000.0, 37000.0, 8000.0, 35000.0, 25000.0, 18000.0, 30000.0, 12000.0, 18000.0, 20000.0, 31000.0, 6000.0, 12000.0, 13000.0, 26000.0, 15000.0, 30000.0, 2000.0, 5000.0, 53000.0, 2000.0, 25000.0, 5000.0, 21000.0, 17000.0, 12000.0, 42000.0, 21000.0, 25000.0, 3000.0, 4000.0, 20000.0, 52000.0, 60000.0, 15000.0, 90000.0, 2000.0, 16000.0, 24000.0, 15000.0, 42000.0, 10000.0, 17000.0, 30000.0, 32000.0, 36000.0, 10000.0, 20000.0, 24000.0, 17000.0, 23000.0, 25000.0, 50000.0, 20000.0, 30000.0, 7000.0, 30000.0, 3000.0, 20000.0, 40000.0, 16000.0, 11000.0, 16000.0, 18000.0, 12000.0, 4000.0, 60000.0, 43000.0, 31000.0, 50000.0, 27000.0, 30000.0, 12000.0, 20000.0, 25000.0, 3000.0, 110000.0, 58000.0, 10000.0, 19000.0, 8000.0, 60000.0, 6000.0], dtype=np.float32).reshape(1059,1)
earn=torch.tensor(earn)
height= np.array([74.0, 66.0, 64.0, 63.0, 64.0, 62.0, 73.0, 72.0, 72.0, 68.0, 68.0, 65.0, 66.0, 68.0, 68.0, 70.0, 64.0, 73.0, 62.0, 63.0, 67.0, 66.0, 72.0, 63.0, 64.0, 72.0, 77.0, 64.0, 64.0, 72.0, 68.0, 64.0, 67.0, 65.0, 64.0, 65.0, 65.0, 67.0, 68.0, 60.0, 65.0, 62.0, 72.0, 67.0, 70.0, 68.0, 71.0, 71.0, 73.0, 68.0, 66.0, 69.0, 63.0, 62.0, 64.0, 67.0, 71.0, 60.0, 66.0, 66.0, 65.0, 70.0, 62.0, 72.0, 67.0, 68.0, 70.0, 62.0, 63.0, 66.0, 70.0, 68.0, 70.0, 66.0, 68.0, 70.0, 61.0, 64.0, 69.0, 64.0, 67.0, 69.0, 67.0, 68.0, 63.0, 71.0, 64.0, 68.0, 68.0, 66.0, 77.0, 67.0, 62.0, 67.0, 61.0, 64.0, 62.0, 63.0, 68.0, 64.0, 72.0, 64.0, 61.0, 74.0, 70.0, 71.0, 63.0, 64.0, 62.0, 70.0, 71.0, 71.0, 67.0, 64.0, 67.0, 62.0, 68.0, 62.0, 68.0, 74.0, 68.0, 62.0, 66.0, 68.0, 60.0, 68.0, 72.0, 66.0, 68.0, 68.0, 69.0, 64.0, 65.0, 61.0, 72.0, 66.0, 68.0, 70.0, 71.0, 71.0, 68.0, 60.0, 63.0, 68.0, 64.0, 68.0, 61.0, 68.0, 66.0, 70.0, 71.0, 65.0, 66.0, 70.0, 66.0, 61.0, 72.0, 74.0, 67.0, 66.0, 62.0, 72.0, 65.0, 60.0, 62.0, 66.0, 64.0, 65.0, 73.0, 69.0, 64.0, 67.0, 62.0, 63.0, 67.0, 68.0, 72.0, 70.0, 70.0, 67.0, 63.0, 66.0, 66.0, 71.0, 67.0, 76.0, 69.0, 65.0, 62.0, 62.0, 62.0, 61.0, 65.0, 72.0, 72.0, 64.0, 71.0, 72.0, 66.0, 65.0, 62.0, 66.0, 67.0, 64.0, 63.0, 67.0, 68.0, 64.0, 66.0, 65.0, 65.0, 72.0, 68.0, 74.0, 63.0, 64.0, 68.0, 67.0, 67.0, 73.0, 65.0, 77.0, 68.0, 62.0, 64.0, 67.0, 70.0, 62.0, 63.0, 62.0, 69.0, 64.0, 69.0, 59.0, 66.0, 62.0, 75.0, 63.0, 65.0, 70.0, 68.0, 69.0, 72.0, 64.0, 63.0, 73.0, 75.0, 65.0, 66.0, 66.0, 64.0, 66.0, 64.0, 63.0, 73.0, 72.0, 72.0, 64.0, 64.0, 68.0, 69.0, 60.0, 76.0, 72.0, 62.0, 62.0, 71.0, 74.0, 65.0, 63.0, 67.0, 63.0, 65.0, 65.0, 63.0, 70.0, 68.0, 67.0, 65.0, 69.0, 68.0, 66.0, 71.0, 64.0, 74.0, 62.0, 65.0, 66.0, 67.0, 73.0, 76.0, 63.0, 63.0, 70.0, 71.0, 65.0, 63.0, 64.0, 64.0, 67.0, 66.0, 68.0, 65.0, 67.0, 73.0, 63.0, 71.0, 73.0, 65.0, 59.0, 70.0, 72.0, 66.0, 70.0, 69.0, 68.0, 74.0, 67.0, 64.0, 67.0, 70.0, 64.0, 62.0, 68.0, 72.0, 63.0, 69.0, 64.0, 71.0, 64.0, 61.0, 68.0, 68.0, 64.0, 66.0, 62.0, 65.0, 66.0, 72.0, 70.0, 62.0, 71.0, 70.0, 64.0, 63.0, 65.0, 71.0, 62.0, 68.0, 76.0, 64.0, 66.0, 63.0, 63.0, 70.0, 68.0, 70.0, 69.0, 63.0, 65.0, 69.0, 68.0, 66.0, 65.0, 70.0, 70.0, 66.0, 68.0, 68.0, 70.0, 65.0, 64.0, 65.0, 77.0, 70.0, 64.0, 64.0, 64.0, 64.0, 63.0, 58.0, 62.0, 64.0, 64.0, 62.0, 64.0, 66.0, 69.0, 64.0, 71.0, 68.0, 68.0, 62.0, 63.0, 69.0, 67.0, 64.0, 70.0, 69.0, 69.0, 69.0, 60.0, 66.0, 67.0, 66.0, 70.0, 64.0, 60.0, 68.0, 73.0, 72.0, 64.0, 67.0, 71.0, 73.0, 70.0, 70.0, 71.0, 66.0, 75.0, 60.0, 72.0, 75.0, 60.0, 73.0, 65.0, 67.0, 69.0, 64.0, 70.0, 68.0, 62.0, 64.0, 68.0, 64.0, 73.0, 73.0, 69.0, 68.0, 71.0, 66.0, 66.0, 70.0, 66.0, 63.0, 75.0, 67.0, 63.0, 69.0, 71.0, 62.0, 65.0, 72.0, 65.0, 63.0, 64.0, 72.0, 64.0, 70.0, 65.0, 64.0, 64.0, 67.0, 73.0, 73.0, 65.0, 64.0, 67.0, 67.0, 74.0, 63.0, 74.0, 74.0, 63.0, 62.0, 62.0, 67.0, 69.0, 73.0, 63.0, 67.0, 69.0, 71.0, 72.0, 64.0, 64.0, 74.0, 62.0, 65.0, 60.0, 69.0, 61.0, 65.0, 66.0, 70.0, 72.0, 64.0, 71.0, 73.0, 64.0, 68.0, 64.0, 66.0, 67.0, 64.0, 64.0, 62.0, 69.0, 63.0, 72.0, 75.0, 70.0, 69.0, 64.0, 72.0, 69.0, 71.0, 59.0, 68.0, 77.0, 70.0, 69.0, 63.0, 73.0, 66.0, 72.0, 72.0, 69.0, 67.0, 66.0, 67.0, 68.0, 65.0, 62.0, 69.0, 70.0, 66.0, 66.0, 66.0, 72.0, 69.0, 66.0, 69.0, 60.0, 65.0, 62.0, 67.0, 67.0, 68.0, 66.0, 74.0, 65.0, 68.0, 66.0, 65.0, 62.0, 73.0, 62.0, 70.0, 58.0, 68.0, 67.0, 70.0, 63.0, 66.0, 65.0, 61.0, 61.0, 67.0, 73.0, 71.0, 64.0, 67.0, 62.0, 67.0, 69.0, 66.0, 66.0, 66.0, 66.0, 72.0, 64.0, 65.0, 72.0, 71.0, 64.0, 64.0, 67.0, 75.0, 66.0, 67.0, 71.0, 68.0, 67.0, 71.0, 63.0, 61.0, 65.0, 67.0, 65.0, 66.0, 67.0, 74.0, 74.0, 66.0, 69.0, 66.0, 74.0, 72.0, 66.0, 67.0, 65.0, 65.0, 68.0, 64.0, 64.0, 63.0, 69.0, 67.0, 60.0, 62.0, 72.0, 67.0, 66.0, 66.0, 60.0, 65.0, 72.0, 64.0, 64.0, 64.0, 66.0, 73.0, 74.0, 65.0, 63.0, 63.0, 66.0, 70.0, 66.0, 63.0, 63.0, 69.0, 73.0, 68.0, 64.0, 63.0, 70.0, 64.0, 66.0, 72.0, 64.0, 67.0, 66.0, 71.0, 71.0, 71.0, 74.0, 75.0, 72.0, 66.0, 65.0, 72.0, 60.0, 64.0, 65.0, 65.0, 62.0, 73.0, 68.0, 66.0, 71.0, 73.0, 61.0, 63.0, 64.0, 67.0, 63.0, 73.0, 66.0, 70.0, 68.0, 67.0, 74.0, 64.0, 60.0, 59.0, 75.0, 64.0, 71.0, 63.0, 67.0, 69.0, 70.0, 66.0, 74.0, 65.0, 64.0, 68.0, 66.0, 69.0, 60.0, 75.0, 64.0, 63.0, 64.0, 63.0, 67.0, 66.0, 65.0, 63.0, 66.0, 64.0, 71.0, 71.0, 72.0, 70.0, 74.0, 68.0, 74.0, 75.0, 68.0, 63.0, 72.0, 64.0, 63.0, 66.0, 61.0, 73.0, 65.0, 62.0, 68.0, 68.0, 63.0, 63.0, 68.0, 64.0, 70.0, 70.0, 72.0, 69.0, 63.0, 63.0, 63.0, 73.0, 69.0, 70.0, 65.0, 72.0, 62.0, 72.0, 73.0, 68.0, 66.0, 60.0, 65.0, 63.0, 64.0, 71.0, 67.0, 63.0, 65.0, 63.0, 65.0, 74.0, 67.0, 68.0, 65.0, 64.0, 68.0, 65.0, 72.0, 66.0, 64.0, 72.0, 64.0, 67.0, 68.0, 64.0, 66.0, 65.0, 70.0, 70.0, 66.0, 66.0, 74.0, 62.0, 66.0, 65.0, 71.0, 66.0, 67.0, 61.0, 66.0, 72.0, 69.0, 64.0, 63.0, 60.0, 70.0, 73.0, 65.0, 64.0, 72.0, 67.0, 69.0, 71.0, 66.0, 63.0, 71.0, 72.0, 68.0, 72.0, 66.0, 66.0, 65.0, 71.0, 74.0, 70.0, 64.0, 69.0, 63.0, 68.0, 73.0, 63.0, 71.0, 65.0, 65.0, 71.0, 70.0, 68.0, 66.0, 64.0, 66.0, 70.0, 64.0, 71.0, 64.0, 63.0, 66.0, 68.0, 62.0, 69.0, 64.0, 64.0, 69.0, 74.0, 68.0, 67.0, 72.0, 73.0, 64.0, 68.0, 63.0, 71.0, 64.0, 60.0, 66.0, 59.0, 63.0, 66.0, 65.0, 64.0, 66.0, 66.0, 73.0, 72.0, 74.0, 65.0, 71.0, 68.0, 70.0, 64.0, 59.0, 68.0, 68.0, 69.0, 69.0, 75.0, 69.0, 72.0, 68.0, 72.0, 63.0, 69.0, 63.0, 72.0, 65.0, 73.0, 69.0, 69.0, 61.0, 73.0, 67.0, 63.0, 62.0, 65.0, 72.0, 67.0, 65.0, 72.0, 64.0, 68.0, 68.0, 76.0, 63.0, 68.0, 72.0, 64.0, 68.0, 61.0, 67.0, 63.0, 60.0, 74.0, 62.0, 72.0, 65.0, 62.0, 71.0, 71.0, 70.0, 67.0, 62.0, 63.0, 61.0, 72.0, 63.0, 66.0, 64.0, 66.0, 68.0, 60.0, 61.0, 64.0, 66.0, 66.0, 67.0, 68.0, 66.0, 69.0, 69.0, 71.0, 74.0, 69.0, 72.0, 64.0, 72.0, 63.0, 70.0, 66.0, 62.0, 64.0, 69.0, 64.0, 67.0, 72.0, 65.0, 63.0, 70.0, 64.0, 68.0, 66.0, 70.0, 66.0, 64.0, 70.0, 74.0, 66.0, 62.0, 66.0, 62.0, 69.0, 68.0, 72.0, 64.0, 72.0, 74.0, 66.0, 68.0, 62.0, 71.0, 63.0, 71.0, 72.0, 72.0, 68.0, 64.0, 70.0, 66.0, 67.0, 68.0, 64.0, 65.0, 70.0, 71.0, 62.0, 71.0, 74.0, 64.0, 70.0, 64.0, 70.0, 74.0, 74.0, 66.0, 71.0, 69.0, 60.0, 64.0, 64.0, 72.0, 69.0, 67.0, 65.0, 68.0, 74.0, 65.0, 71.0, 64.0, 69.0, 64.0, 72.0, 63.0, 65.0, 72.0, 65.0, 64.0, 71.0, 62.0, 66.0, 64.0, 68.0, 66.0, 66.0, 66.0, 66.0, 63.0, 68.0, 70.0, 63.0, 70.0, 72.0, 75.0, 62.0, 68.0, 68.0, 70.0, 73.0, 62.0, 64.0, 73.0, 66.0, 70.0, 70.0, 72.0, 64.0, 72.0, 68.0], dtype=np.float32).reshape(1059,1)
height=torch.tensor(height)
eth= np.array([3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 1, 1, 3, 4, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 4, 3, 3, 3, 3, 1, 1, 4, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 1, 1, 4, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 4, 3, 1, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 1, 2, 2, 3, 1, 3, 1, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 4, 1, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 1, 3, 2, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 2, 3, 4, 3, 3, 4, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 4, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 1, 2, 3, 4, 2, 4, 3, 1, 2, 3, 1, 3, 2, 3, 3, 3, 1, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 4, 2, 4, 3, 3, 3, 3, 3, 3], dtype=np.int64).reshape(1059,1)
eth=torch.tensor(eth)
N=1059
N=torch.tensor(N)
def model(earn,height,eth,N):
    log_earn = torch.zeros([amb(N)])
    log_earn=torch.log(earn)
    xi = pyro.sample('xi'.format(''), dist.Normal(torch.tensor(1234.0)*torch.ones([amb(1)]),torch.tensor(1234.0)*torch.ones([amb(1)])))
    mu_a1 = pyro.sample('mu_a1'.format(''), dist.Normal(torch.tensor(0.0)*torch.ones([amb(1)]),torch.tensor(1.0)*torch.ones([amb(1)])))
    mu_a2 = pyro.sample('mu_a2'.format(''), dist.Normal(torch.tensor(0.0)*torch.ones([amb(1)]),torch.tensor(1.0)*torch.ones([amb(1)])))
    with pyro.iarange('eta1_range_'.format('')):
        eta1 = pyro.sample('eta1'.format(''), dist.Normal(torch.tensor(0.0)*torch.ones([amb(4)]),torch.tensor(1.0)*torch.ones([amb(4)])))
    with pyro.iarange('eta2_range_'.format('')):
        eta2 = pyro.sample('eta2'.format(''), dist.Normal(torch.tensor(0.0)*torch.ones([amb(4)]),torch.tensor(1.0)*torch.ones([amb(4)])))
    sigma_a1 = pyro.sample('sigma_a1'.format(''), dist.Cauchy(torch.tensor(0.0)*torch.ones([amb(1)]),torch.tensor(5.0)*torch.ones([amb(1)])))
    sigma_a2 = pyro.sample('sigma_a2'.format(''), dist.Cauchy(torch.tensor(0.0)*torch.ones([amb(1)]),torch.tensor(5.0)*torch.ones([amb(1)])))
    a1 = torch.zeros([amb(4)])
    a2 = torch.zeros([amb(4)])
    y_hat = torch.zeros([amb(N)])
    a1=10*mu_a1+sigma_a1*eta1
    a2=0.1*mu_a2+sigma_a2*eta2
    for i in range(1, N+1):
        y_hat[i-1]=a1[eth[i-1]-1]+a2[eth[i-1]-1]*height[i-1]
    sigma_y = pyro.sample('sigma_y'.format(''), dist.Cauchy(torch.tensor(0.0)*torch.ones([amb(1)]),torch.tensor(5.0)*torch.ones([amb(1)])))
    log_earn=dist.Normal(y_hat,sigma_y)
    
def guide(earn,height,eth,N):
    arg_1 = pyro.param('arg_1', torch.ones((amb(1))), constraint=constraints.positive)
    arg_2 = pyro.param('arg_2', torch.ones((amb(1))), constraint=constraints.positive)
    xi = pyro.sample('xi'.format(''), dist.Gamma(arg_1,arg_2))
    arg_3 = pyro.param('arg_3', torch.ones((amb(1))), constraint=constraints.positive)
    arg_4 = pyro.param('arg_4', torch.ones((amb(1))), constraint=constraints.positive)
    mu_a1 = pyro.sample('mu_a1'.format(''), dist.Gamma(arg_3,arg_4))
    arg_5 = pyro.param('arg_5', torch.ones((amb(1))))
    arg_6 = pyro.param('arg_6', torch.ones((amb(1))), constraint=constraints.positive)
    mu_a2 = pyro.sample('mu_a2'.format(''), dist.Normal(arg_5,arg_6))
    arg_7 = pyro.param('arg_7', torch.ones((amb(4))), constraint=constraints.positive)
    arg_8 = pyro.param('arg_8', torch.ones((amb(4))), constraint=constraints.positive)
    with pyro.iarange('eta1_prange'):
        eta1 = pyro.sample('eta1'.format(''), dist.Beta(arg_7,arg_8))
    arg_9 = pyro.param('arg_9', torch.ones((amb(4))), constraint=constraints.positive)
    arg_10 = pyro.param('arg_10', torch.ones((amb(4))), constraint=constraints.positive)
    with pyro.iarange('eta2_prange'):
        eta2 = pyro.sample('eta2'.format(''), dist.Weibull(arg_9,arg_10))
    arg_11 = pyro.param('arg_11', torch.ones((amb(1))), constraint=constraints.positive)
    arg_12 = pyro.param('arg_12', torch.ones((amb(1))), constraint=constraints.positive)
    sigma_a1 = pyro.sample('sigma_a1'.format(''), dist.Gamma(arg_11,arg_12))
    arg_13 = pyro.param('arg_13', torch.ones((amb(1))), constraint=constraints.positive)
    arg_14 = pyro.param('arg_14', torch.ones((amb(1))), constraint=constraints.positive)
    sigma_a2 = pyro.sample('sigma_a2'.format(''), dist.Weibull(arg_13,arg_14))
    for i in range(1, N+1):
        pass
    arg_15 = pyro.param('arg_15', torch.ones((amb(1))), constraint=constraints.positive)
    sigma_y = pyro.sample('sigma_y'.format(''), dist.Exponential(arg_15))
    
    pass
    return { "sigma_y": sigma_y,"xi": xi,"eta1": eta1,"eta2": eta2,"sigma_a2": sigma_a2,"sigma_a1": sigma_a1,"mu_a2": mu_a2,"mu_a1": mu_a1, }
optim = Adam({'lr': 0.05})
svi = SVI(model, guide, optim, loss=Trace_ELBO() if pyro.__version__ > '0.1.2' else 'ELBO')
for i in range(4000):
    loss = svi.step(earn,height,eth,N)
    if ((i % 1000) == 0):
        print(loss)
for name in pyro.get_param_store().get_all_param_names():
    print(('{0} : {1}'.format(name, pyro.param(name).data.numpy())))
print('sigma_y_mean', np.array2string(dist.Exponential(pyro.param('arg_15')).mean.detach().numpy(), separator=','))
print('xi_mean', np.array2string(dist.Gamma(pyro.param('arg_1'), pyro.param('arg_2')).mean.detach().numpy(), separator=','))
print('eta1_mean', np.array2string(dist.Beta(pyro.param('arg_7'), pyro.param('arg_8')).mean.detach().numpy(), separator=','))
print('eta2_mean', np.array2string(dist.Weibull(pyro.param('arg_9'), pyro.param('arg_10')).mean.detach().numpy(), separator=','))
print('sigma_a2_mean', np.array2string(dist.Weibull(pyro.param('arg_13'), pyro.param('arg_14')).mean.detach().numpy(), separator=','))
print('sigma_a1_mean', np.array2string(dist.Gamma(pyro.param('arg_11'), pyro.param('arg_12')).mean.detach().numpy(), separator=','))
print('mu_a2_mean', np.array2string(dist.Normal(pyro.param('arg_5'), pyro.param('arg_6')).mean.detach().numpy(), separator=','))
print('mu_a1_mean', np.array2string(dist.Gamma(pyro.param('arg_3'), pyro.param('arg_4')).mean.detach().numpy(), separator=','))
np.set_printoptions(threshold=np.inf)
with open('samples','w') as samplefile:
    samplefile.write('sigma_y:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['sigma_y'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('xi:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['xi'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('eta1:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['eta1'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('eta2:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['eta2'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('sigma_a2:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['sigma_a2'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('sigma_a1:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['sigma_a1'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('mu_a2:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['mu_a2'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
    samplefile.write('mu_a1:')
    samplefile.write(np.array2string(np.array([guide(earn,height,eth,N)['mu_a1'].data.numpy() for _ in range(1000)]), separator=',').replace('\n',''))
    samplefile.write('\n')
